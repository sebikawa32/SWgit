import json
import time
import re

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

from detail_crawler import crawl_detail
from db_insert import insert_to_mysql

def main():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    driver = webdriver.Chrome(options=options)

    categories = {
        "ì—°ê·¹": ("https://www.ticketlink.co.kr/performance/15", 3),
        "ë®¤ì§€ì»¬": ("https://www.ticketlink.co.kr/performance/16", 4),
        "ì½˜ì„œíŠ¸": ("https://www.ticketlink.co.kr/performance/14", 1),
        "ì „ì‹œ": ("https://www.ticketlink.co.kr/exhibition/11", 2)
    }

    for category_name, (url, category_id) in categories.items():
        print(f"\nâ–¶ ì¹´í…Œê³ ë¦¬: {category_name}")
        driver.get(url)

        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "ul.product_grid_list > li.product_grid_item"))
            )
        except Exception as e:
            print(f"âŒ ê³µì—° ëª©ë¡ ë¡œë”© ì‹¤íŒ¨: {url} | ì—ëŸ¬: {e}")
            continue

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        performance_list = soup.select('ul.product_grid_list > li.product_grid_item')
        print("ê³µì—° ê°œìˆ˜:", len(performance_list))

        for perf in performance_list:
            try:
                # ê³µì—° ì œëª©
                title_tag = perf.select_one('.product_title')
                title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

                # ë§í¬ ì¶”ì¶œ
                link_tag = perf.select_one('a')
                if link_tag and link_tag.has_attr('href'):
                    href = link_tag['href']
                    detail_url = href if href.startswith("http") else "https://www.ticketlink.co.kr" + href
                else:
                    onclick_attr = perf.get("onclick")
                    if onclick_attr and "location.href" in onclick_attr:
                        href_match = re.search(r"location\.href\s*=\s*['\"]([^'\"]+)['\"]", onclick_attr)
                        if href_match:
                            href = href_match.group(1)
                            detail_url = "https://www.ticketlink.co.kr" + href
                        else:
                            print("âŒ onclickì—ì„œ href ì¶”ì¶œ ì‹¤íŒ¨")
                            continue
                    else:
                        print("âŒ ë§í¬ ì—†ìŒ, ê±´ë„ˆëœ€")
                        continue

                # ìƒì„¸ í˜ì´ì§€ â†’ í¬ë¡¤ë§
                detail_data = crawl_detail(detail_url, driver)
                detail_data["category_id"] = category_id

                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (DBì— ë„£ì„ ë•Œ ë¬¸ì œ ë°©ì§€ìš©)
                if isinstance(detail_data.get("ticket_description_url"), list):
                    detail_data["ticket_description_url"] = json.dumps(detail_data["ticket_description_url"])

                # DB ì €ì¥
                insert_to_mysql(detail_data)

                time.sleep(0.5)

            except Exception as e:
                print(f"âŒ í•˜ë‚˜ì˜ ê³µì—° ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    driver.quit()
    print("\nâœ… ì „ì²´ í¬ë¡¤ë§ ë° DB ì €ì¥ ì™„ë£Œ!")

def lambda_handler(event, context):
    try:
        print("ğŸš€ Lambda í•¨ìˆ˜ ì‹œì‘ë¨")
        main()
        return {
            'statusCode': 200,
            'body': json.dumps('âœ… Lambdaì—ì„œ í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!')
        }
    except Exception as e:
        print("âŒ Lambda ì˜¤ë¥˜:", str(e))
        return {
            'statusCode': 500,
            'body': json.dumps(f'âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}')
        }

if __name__ == "__main__":
    main()
